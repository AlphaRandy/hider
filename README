<metadata>
<identifier>fhnzugv21mka72tugbvkkvemjkbbdpbeilb65hgm</identifier>
<creator>Changelog Master Feed</creator>
<date>2022-01-11</date>
<description>We have all seen how AI models fail, sometimes in spectacular ways. Yaron Singer joins us in this episode to discuss model vulnerabilities and automatic prevention of bad outcomes. By separating concerns and creating a \"firewall\" around your AI models, it's possible to secure your AI workflows and prevent model failure.</description>
<guid>changelog.com/7/1554</guid>
<mediatype>audio</mediatype>
<rssfeed>https://changelog.com/master/feed</rssfeed>
<scanner>Internet Archive Python library 1.9.6</scanner>
<sessionid>y3fr0oeajtlrs14-</sessionid>
<subject>Podcast</subject>
<subject>changelog</subject>
<subject> open source</subject>
<subject> oss</subject>
<subject> software</subject>
<subject> development</subject>
<subject> developer</subject>
<subject> hackerchangelog</subject>
<subject> ai</subject>
<subject> machine learning</subject>
<subject> deep learning</subject>
<subject> artificial intelligence</subject>
<subject> neural networks</subject>
<subject> computer vision</subject>
<title>Eliminate AI failures (Practical AI #163)</title>
<uploadsoftware>LifePod-Beta</uploadsoftware>
<collection>podcasts_mirror_bobarchives</collection>
<uploader>bobarchives123@gmail.com</uploader>
<publicdate>2022-01-12 02:37:23</publicdate>
<addeddate>2022-01-12 02:37:23</addeddate>
<curation>[curator]validator@archive.org[/curator][date]20220112024040[/date][comment]checked for malware[/comment]</curation>
<collection>podcasts_mirror</collection>
<access-restricted-item>true</access-restricted-item>
</metadata>

